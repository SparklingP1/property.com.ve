name: Scrape Properties (Distributed)

on:
  # Run every Sunday at 3am UTC
  schedule:
    - cron: '0 3 * * 0'

  # Allow manual trigger from GitHub UI
  workflow_dispatch:
    inputs:
      job_count:
        description: 'Number of parallel jobs (default: 9)'
        required: false
        default: '9'
        type: choice
        options:
          - '1'
          - '3'
          - '5'
          - '9'
          - '12'

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 360  # 6 hours per job
    environment: Production

    strategy:
      max-parallel: 10  # Run up to 10 jobs concurrently
      fail-fast: false  # Continue other jobs if one fails

      matrix:
        # Split 1,284 pages across 9 jobs (~143 pages each)
        # Each job takes ~6 hours at 2.5 min/page
        page_range:
          - { start: 1, end: 143, name: "Job 1/9" }
          - { start: 144, end: 286, name: "Job 2/9" }
          - { start: 287, end: 429, name: "Job 3/9" }
          - { start: 430, end: 572, name: "Job 4/9" }
          - { start: 573, end: 715, name: "Job 5/9" }
          - { start: 716, end: 858, name: "Job 6/9" }
          - { start: 859, end: 1001, name: "Job 7/9" }
          - { start: 1002, end: 1144, name: "Job 8/9" }
          - { start: 1145, end: 1284, name: "Job 9/9" }

    name: ${{ matrix.page_range.name }} (Pages ${{ matrix.page_range.start }}-${{ matrix.page_range.end }})

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: scraper/requirements.txt

      - name: Install dependencies
        run: pip install -r scraper/requirements.txt

      - name: Install Playwright browsers
        run: playwright install chromium

      - name: Run scraper for pages ${{ matrix.page_range.start }}-${{ matrix.page_range.end }}
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
        run: |
          python scraper/run.py \
            --start-page ${{ matrix.page_range.start }} \
            --end-page ${{ matrix.page_range.end }}

      - name: Report completion
        if: always()
        run: |
          echo "=============================================="
          echo "âœ… Completed ${{ matrix.page_range.name }}"
          echo "   Pages: ${{ matrix.page_range.start }}-${{ matrix.page_range.end }}"
          echo "   Status: ${{ job.status }}"
          echo "   Time: $(date)"
          echo "=============================================="

  summary:
    name: Scrape Summary
    needs: scrape
    runs-on: ubuntu-latest
    if: always()

    steps:
      - name: Check scrape results
        run: |
          echo "=============================================="
          echo "ðŸ“Š DISTRIBUTED SCRAPE SUMMARY"
          echo "=============================================="
          echo "Total pages: 1,284"
          echo "Jobs: 9 parallel"
          echo "Status: ${{ needs.scrape.result }}"
          echo ""
          echo "Next steps:"
          echo "- Check Supabase dashboard for listing count"
          echo "- Verify ~15,405 active listings"
          echo "- Review any failed jobs above"
          echo "=============================================="
