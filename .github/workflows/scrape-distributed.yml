name: Scrape Properties (Distributed)

on:
  # Run every Sunday at 3am UTC
  schedule:
    - cron: '0 3 * * 0'

  # Allow manual trigger from GitHub UI
  workflow_dispatch:
    inputs:
      job_count:
        description: 'Number of parallel jobs (default: 9)'
        required: false
        default: '9'
        type: choice
        options:
          - '1'
          - '3'
          - '5'
          - '9'
          - '12'

jobs:
  detect:
    name: Detect Total Pages
    runs-on: ubuntu-latest
    timeout-minutes: 5
    outputs:
      total_pages: ${{ steps.calculate.outputs.total }}
      page_ranges: ${{ steps.calculate.outputs.ranges }}
      job_count: ${{ steps.calculate.outputs.jobs }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install detection dependencies
        run: pip install playwright beautifulsoup4 lxml

      - name: Install Playwright browsers
        run: playwright install chromium

      - name: Detect page count and calculate ranges
        id: calculate
        run: |
          echo "ðŸ” Calculating page ranges..."
          # Full scrape with optimized settings (18 jobs for faster completion)
          TOTAL=1282
          START=1

          # Calculate ranges for ALL pages (1-1282 across 18 jobs, ~71 pages each)
          RANGES='[{"start":1,"end":71,"name":"Job 1/18"},{"start":72,"end":142,"name":"Job 2/18"},{"start":143,"end":213,"name":"Job 3/18"},{"start":214,"end":284,"name":"Job 4/18"},{"start":285,"end":355,"name":"Job 5/18"},{"start":356,"end":426,"name":"Job 6/18"},{"start":427,"end":497,"name":"Job 7/18"},{"start":498,"end":568,"name":"Job 8/18"},{"start":569,"end":639,"name":"Job 9/18"},{"start":640,"end":710,"name":"Job 10/18"},{"start":711,"end":781,"name":"Job 11/18"},{"start":782,"end":852,"name":"Job 12/18"},{"start":853,"end":923,"name":"Job 13/18"},{"start":924,"end":994,"name":"Job 14/18"},{"start":995,"end":1065,"name":"Job 15/18"},{"start":1066,"end":1136,"name":"Job 16/18"},{"start":1137,"end":1207,"name":"Job 17/18"},{"start":1208,"end":1282,"name":"Job 18/18"}]'

          echo "total=$TOTAL" >> $GITHUB_OUTPUT
          echo "ranges=$RANGES" >> $GITHUB_OUTPUT
          echo "jobs=18" >> $GITHUB_OUTPUT

          echo "âœ… Full scrape pages $START-$TOTAL (1282 pages)"
          echo "ðŸ“Š Split into 18 parallel jobs (~71 pages each)"
          echo "ðŸš€ Optimized: domcontentloaded wait strategy for 2-3x faster page loads"

  scrape:
    name: ${{ matrix.page_range.name }} (Pages ${{ matrix.page_range.start }}-${{ matrix.page_range.end }})
    needs: detect
    runs-on: ubuntu-latest
    timeout-minutes: 360  # 6 hours per job (increased safety margin)
    environment: Production  # Re-enabled - billing issue resolved

    strategy:
      max-parallel: 20  # Run up to 20 jobs concurrently (GitHub free tier limit)
      fail-fast: false  # Continue other jobs if one fails
      matrix:
        page_range: ${{ fromJson(needs.detect.outputs.page_ranges) }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: scraper/requirements.txt

      - name: Install dependencies
        run: pip install -r scraper/requirements.txt

      - name: Install Playwright browsers
        run: playwright install chromium

      - name: Run scraper for pages ${{ matrix.page_range.start }}-${{ matrix.page_range.end }}
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
        run: |
          python scraper/run.py \
            --start-page ${{ matrix.page_range.start }} \
            --end-page ${{ matrix.page_range.end }}

      - name: Report completion
        if: always()
        run: |
          echo "=============================================="
          echo "âœ… Completed ${{ matrix.page_range.name }}"
          echo "   Pages: ${{ matrix.page_range.start }}-${{ matrix.page_range.end }}"
          echo "   Status: ${{ job.status }}"
          echo "   Time: $(date)"
          echo "=============================================="

  summary:
    name: Scrape Summary
    needs: [detect, scrape]
    runs-on: ubuntu-latest
    if: always()

    steps:
      - name: Check scrape results
        run: |
          echo "=============================================="
          echo "ðŸ“Š DISTRIBUTED SCRAPE SUMMARY"
          echo "=============================================="
          echo "Total pages detected: ${{ needs.detect.outputs.total_pages }}"
          echo "Jobs: ${{ needs.detect.outputs.job_count }} parallel"
          echo "Status: ${{ needs.scrape.result }}"
          echo ""
          echo "Next steps:"
          echo "- Check Supabase dashboard for listing count"
          echo "- Verify active listings count"
          echo "- Review any failed jobs above"
          echo "=============================================="
