name: Scrape Properties (Distributed)

on:
  # Run every Sunday at 3am UTC
  schedule:
    - cron: '0 3 * * 0'

  # Allow manual trigger from GitHub UI
  workflow_dispatch:
    inputs:
      job_count:
        description: 'Number of parallel jobs (default: 9)'
        required: false
        default: '9'
        type: choice
        options:
          - '1'
          - '3'
          - '5'
          - '9'
          - '12'

jobs:
  detect:
    name: Detect Total Pages
    runs-on: ubuntu-latest
    timeout-minutes: 5
    outputs:
      total_pages: ${{ steps.calculate.outputs.total }}
      page_ranges: ${{ steps.calculate.outputs.ranges }}
      job_count: ${{ steps.calculate.outputs.jobs }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install detection dependencies
        run: pip install playwright beautifulsoup4 lxml

      - name: Install Playwright browsers
        run: playwright install chromium

      - name: Detect page count and calculate ranges
        id: calculate
        run: |
          echo "üîç Calculating page ranges..."
          # Start from page 102 (pages 1-101 already scraped)
          TOTAL=1282
          START=102

          # Calculate ranges for remaining pages (102-1282 = 1181 pages across 9 jobs)
          RANGES='[{"start":102,"end":232,"name":"Job 1/9"},{"start":233,"end":363,"name":"Job 2/9"},{"start":364,"end":494,"name":"Job 3/9"},{"start":495,"end":625,"name":"Job 4/9"},{"start":626,"end":756,"name":"Job 5/9"},{"start":757,"end":887,"name":"Job 6/9"},{"start":888,"end":1018,"name":"Job 7/9"},{"start":1019,"end":1149,"name":"Job 8/9"},{"start":1150,"end":1282,"name":"Job 9/9"}]'

          echo "total=$TOTAL" >> $GITHUB_OUTPUT
          echo "ranges=$RANGES" >> $GITHUB_OUTPUT
          echo "jobs=9" >> $GITHUB_OUTPUT

          echo "‚úÖ Scraping pages $START-$TOTAL (1181 pages)"
          echo "üìä Split into 9 parallel jobs (~131 pages each)"
          echo "‚è≠Ô∏è  Skipping pages 1-101 (already scraped)"

  scrape:
    name: ${{ matrix.page_range.name }} (Pages ${{ matrix.page_range.start }}-${{ matrix.page_range.end }})
    needs: detect
    runs-on: ubuntu-latest
    timeout-minutes: 360  # 6 hours per job
    environment: Production  # Re-enabled - billing issue resolved

    strategy:
      max-parallel: 10  # Run up to 10 jobs concurrently
      fail-fast: false  # Continue other jobs if one fails
      matrix:
        page_range: ${{ fromJson(needs.detect.outputs.page_ranges) }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: scraper/requirements.txt

      - name: Install dependencies
        run: pip install -r scraper/requirements.txt

      - name: Install Playwright browsers
        run: playwright install chromium

      - name: Run scraper for pages ${{ matrix.page_range.start }}-${{ matrix.page_range.end }}
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
        run: |
          python scraper/run.py \
            --start-page ${{ matrix.page_range.start }} \
            --end-page ${{ matrix.page_range.end }}

      - name: Report completion
        if: always()
        run: |
          echo "=============================================="
          echo "‚úÖ Completed ${{ matrix.page_range.name }}"
          echo "   Pages: ${{ matrix.page_range.start }}-${{ matrix.page_range.end }}"
          echo "   Status: ${{ job.status }}"
          echo "   Time: $(date)"
          echo "=============================================="

  summary:
    name: Scrape Summary
    needs: [detect, scrape]
    runs-on: ubuntu-latest
    if: always()

    steps:
      - name: Check scrape results
        run: |
          echo "=============================================="
          echo "üìä DISTRIBUTED SCRAPE SUMMARY"
          echo "=============================================="
          echo "Total pages detected: ${{ needs.detect.outputs.total_pages }}"
          echo "Jobs: ${{ needs.detect.outputs.job_count }} parallel"
          echo "Status: ${{ needs.scrape.result }}"
          echo ""
          echo "Next steps:"
          echo "- Check Supabase dashboard for listing count"
          echo "- Verify active listings count"
          echo "- Review any failed jobs above"
          echo "=============================================="
